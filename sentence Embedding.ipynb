{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 링크\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec, Glove 등 단어에 대한 임베딩(벡터화)는 쉬운데 , 문장에 대한 벡터화에 어려움을 겪어 공부해봄\n",
    "- 문장 벡터화의 경우는 크게 4가지 방법이 존재함\n",
    "    - 1. Doc2Vec\n",
    "    - 2. sentence Bert (sBert)\n",
    "    - 3. InferSent\n",
    "    - 4. Universal Sentence Encoder\n",
    "    \n",
    "\n",
    "이 외에도 TF-IDF, 문장에 단어들 토큰화 후 , 문장 내의 단어들을 average값을 활용하는 방법도 존재 => 문장의 맥락은 이해하기 힘듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: \n",
    "\n",
    "Library Load , 영어 데이터에는 주로  nltk 라이브러리 사용 (natural language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def cosine(u,v ):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2 : \n",
    "예시 문장 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I ate dinner\",\n",
    "             \"We had a three-course meal\",\n",
    "             \"Brad came to dinner with us.\",\n",
    "             \"He loves fish tacos.\",\n",
    "             \"In the end, we all felt like we ate too much\",\n",
    "             \"We all agreed; it was a magnificent evening.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3:\n",
    "\n",
    "문장 데이터 셋을 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'ate', 'dinner'],\n",
       " ['we', 'had', 'a', 'three-course', 'meal'],\n",
       " ['brad', 'came', 'to', 'dinner', 'with', 'us', '.'],\n",
       " ['he', 'loves', 'fish', 'tacos', '.'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'end',\n",
       "  ',',\n",
       "  'we',\n",
       "  'all',\n",
       "  'felt',\n",
       "  'like',\n",
       "  'we',\n",
       "  'ate',\n",
       "  'too',\n",
       "  'much'],\n",
       " ['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent = []\n",
    "\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "    \n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4:\n",
    "Cosine Similarity Funciton 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cosineSim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Doc2Vec</center>\n",
    "- introduced in 2014\n",
    "- unsuervised algorithm and adds on to the Word2Vec model by introducing another 'paragraph vector'. Also there are 2 ways to add the paragraph vector to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/doc2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) __PVDOBW__(Distributed Bag of Words version of Par) : Word2Vec의 Skip-gram 과 유사하게 , 단어가 주어지면 그 단어의 주변값을 예측해가면서 학습하는 과정\n",
    "\n",
    "2) __PVDM(Distributed Memory Version of Paragraph Vector)__ :we predict the next sentence given a set of sentences, 문장의 다음에 나올 단어들을 예측해가면서 학습하는 과정\n",
    "\n",
    "- 글쓴이는 , 이 두가지 방법을 조합하여 사용하는것을 주천하지만  PVDM 이 성능이 더 좋다고 하는듯 ? Word2Vec에서는 보통 Skip-gram이 더 좋은것으로 알고있는데 , 경우에 따라 다른것 같음. Default 값은 PVDOVW 로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': <gensim.models.keyedvectors.Vocab at 0x24029cc5408>,\n",
       " 'ate': <gensim.models.keyedvectors.Vocab at 0x2402a0606c8>,\n",
       " 'dinner': <gensim.models.keyedvectors.Vocab at 0x2402a0da6c8>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x2402815ff08>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x2402815f848>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x240281c65c8>,\n",
       " 'three-course': <gensim.models.keyedvectors.Vocab at 0x240283199c8>,\n",
       " 'meal': <gensim.models.keyedvectors.Vocab at 0x24028319bc8>,\n",
       " 'brad': <gensim.models.keyedvectors.Vocab at 0x2402815fd48>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x2402815fac8>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x24028319908>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x24028319748>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x24028319288>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x24028319248>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x240283194c8>,\n",
       " 'loves': <gensim.models.keyedvectors.Vocab at 0x24028319408>,\n",
       " 'fish': <gensim.models.keyedvectors.Vocab at 0x24028319e88>,\n",
       " 'tacos': <gensim.models.keyedvectors.Vocab at 0x24028319dc8>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x24028319ec8>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x24028319e08>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x24028319fc8>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x24028319c88>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x24028319388>,\n",
       " 'felt': <gensim.models.keyedvectors.Vocab at 0x24028319848>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x24028319f08>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x24028319888>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x24028319608>,\n",
       " 'agreed': <gensim.models.keyedvectors.Vocab at 0x24028319948>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x24028319d48>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x24028319208>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x24028319b88>,\n",
       " 'magnificent': <gensim.models.keyedvectors.Vocab at 0x24028319b48>,\n",
       " 'evening': <gensim.models.keyedvectors.Vocab at 0x24028319708>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word tiwhin a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this\n",
    "alpha = The initial learning rate\n",
    "\n",
    "'''\n",
    "\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0003346   0.02430088  0.01482583  0.01320496  0.00774708 -0.00479163\n",
      " -0.0043848   0.0011541   0.01091491 -0.00402519  0.02941764 -0.0013013\n",
      " -0.00784817  0.0232918  -0.02132636 -0.01285449 -0.00544751 -0.00254209\n",
      " -0.01468766 -0.00313348]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.41944509744644165),\n",
       " (4, 0.37895679473876953),\n",
       " (3, 0.3163576126098633),\n",
       " (1, 0.31625896692276),\n",
       " (2, 0.22266343235969543),\n",
       " (0, 0.20930778980255127)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "\n",
    "print(test_doc_vector) # vector_size = 20으로 설정하였으므로 , 20차원의 벡터 출력됨\n",
    "\n",
    "model.docvecs.most_similar(positive = [test_doc_vector])  # positive = List of sentences that contribute positively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 데이터가 적어서 , 결과적으로 좋은지는 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> sBERT</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence-transformers-0.3.9.tar.gz (64 kB)\n",
      "Collecting transformers<3.6.0,>=3.1.0\n",
      "  Using cached transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3_19_1222\\lib\\site-packages (from sentence-transformers) (4.44.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch>=1.6.0 (from sentence-transformers) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.6.0 (from sentence-transformers)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### error 로 인해 torch 설치가 안되서 수행은 못함, GPU가 없어서 그렇지 않을까 싶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens') #pre-trained Model인듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1 : \n",
    "Encode the provided sentences. We can also display the sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "#print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "#print('Sample embedding vector - note includes negative values', sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 2:\n",
    "Define a test query and decode it as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I had pizza and pasta\"\n",
    "query_vec = model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 3:\n",
    "Compute the cosine similarity using scipy. We will retrieve the similarity values between the sentecnes and our test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    sim = cosine(query_vec, model.encode([sent])[0])\n",
    "    print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
