{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/24949 참고 링크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재 의미 분석(Latent Semantic Analysis , LSA) \n",
    "- LSA 를 수행하기 위해서는 선형대수학의 특이값분해(SVD)의 개념에 대한 이해가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 특이값 분해(Singular Value Decomposition, SVD)\n",
    "\n",
    "- SVD란 , A가 m x n 행렬일때 , 다음과 같이 3개의 행렬곱으로 분해하는 것\n",
    "$$A=UΣV^T$$\n",
    "\n",
    "- 여기서 각 3개의 행렬은 다음과 같은 조건을 만족합니다.\n",
    "$$U:m×m 직교행렬 (AA^T=U(ΣΣT)U^T)$$\n",
    "$$V:n×n 직교행렬 (A^TA=V(ΣTΣ)V^T)$$\n",
    "$$Σ:m×n 직사각 대각행렬$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 절단된 SVD (Truncated SVD)\n",
    "- 위 1번에서는 full SVD라고 불린다.\n",
    "- 하지만 LSA 에서는 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)사용\n",
    "- 아래 그림으로 , 확실히 벡터 차원의 수가 작아짐을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG)\n",
    "- 위 과정을 통해 , 상대적으로 중요하지 않은 정보를 삭제해줌\n",
    "- 계산 비용 또한 낮아진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 잠재 의미 분석 (Latent Semantic Analysis , LSA)\n",
    "-DTM 공부에 활용했던 예제를 다시 사용\n",
    "\n",
    "문서1 : 먹고 싶은 사과<br>\n",
    "문서2 : 먹고 싶은 바나나<br>\n",
    "문서3 : 길고 노란 바나나 바나나<br>\n",
    "문서4 : 저는 과일이 좋아요<br>\n",
    "<center> DTM </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과일이| 길고 |노란  |먹고|바나나|사과|싶은|저는|좋아요\n",
    "---   |---   |---   |--- |---   |--- |--- |--- |--- \n",
    "문서1|\t0   |0\t   |0\t|1\t|0\t|1\t|1\t|0\t|0\n",
    "문서2|\t0   |0\t   |0\t|1\t|1\t|0\t|1\t|0\t|0\n",
    "문서3|\t0\t|1\t   |1\t|0\t|2\t|0\t|0\t|0\t|0\n",
    "문서4|\t1\t|0\t   |0\t|0\t|0\t|0\t|0\t|1\t|1\n",
    "\n",
    "- 위를 바탕으로 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full SVD 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(U.round(2))  #소수점 너무 길이서 두번째까지만 출력\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서 s는 대각행렬이 아닌 , 특이값의 리스트로 변환해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.zeros((4, 9)) # 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S[:4, :4] = np.diag(s) # 특이값을 대각행렬에 삽입\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 × 9의 크기를 가지는 대각 행렬 S가 생성되었습니다.\n",
    "- 2.69 > 2.05 > 1.73 > 0.77 순으로 값이 내림차순을 보이는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 9 × 9의 크기를 가지는 직교 행렬 VT(V의 전치 행렬)가 생성되었습니다. \n",
    "- 즉, U × S × VT를 하면 기존의 행렬 A가 나와야 합니다.\n",
    "- Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴합니다. \n",
    "- 이를 사용하여 정말로 기존의 행렬 A와 동일한지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD 실습\n",
    "- truncated SVD 는 t를 정하는것이 중요함\n",
    "- 여기서 t = 2 로 두기로 함 .\n",
    "- 행렬 S 내의 특이값 중에서 상위2개만 남기고 나머지 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "S=S[:2,:2]\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U=U[:,:2]\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2개의 열만 남기고 모두 제거 \n",
    "- 이제 행렬 V의 전치 행렬인 VT에 대해서 2개 행만 남기고 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VT=VT[:2,:]\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 축소된 행렬 U, S, VT에 대해서 다시 U × S × VT연산을 하면 기존의 A와는 다른 결과가 나오게 됩니다. \n",
    "- 값이 손실되었기 때문에 이 세 개의 행렬로는 이제 기존의 A행렬을 복구할 수 없습니다.\n",
    "- U × S × VT연산을 해서 나오는 값을 A_prime이라 하고 기존의 행렬 A와 값을 비교해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]] \n",
      "\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime=np.dot(np.dot(U,S), VT)\n",
    "print(A,'\\n')\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 차원을 많이 축소시켰는데도 , 대체적으로 비슷한 경향을 띄는것을 볼 수 있다.\n",
    ">- 기존에 0인 값들은 0에 가까운 값이 나오고 , 1인 값들은 1에 나오는 것을볼 수 있다.\n",
    ">- 하지만 , 예외적으로 문제가 있는 부분은 존재한다 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스데이터를 통한 실습\n",
    "- 여기서는 LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력하는 실습으로 토픽 모델링을 수행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 뉴스 데이터에 대한 이해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle = True , random_state=1, remove=('headers','footers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련에 사용할 뉴스는 총 11,314개이고 , 첫번재 훈련용 뉴스 출력해서 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용할 사이킷런에서 제공되는 뉴스데이터는 뉴스별 target_name에 20개의 카테고리가 저장되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 텍스트 전처리\n",
    "- 정제과정을 진행해야함\n",
    "- 정규표현식을 통한 구두점 , 숫자 , 특수 문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document' : documents})\n",
    "#특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "#길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][0]  # 잘 정제되었음을 확인 할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화를 해준 후에 불용어를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "#불용어 제거\n",
    "\n",
    "news_df['clean_doc'] = tokenized_doc.apply(lambda x : \" \".join(x))\n",
    "#불용어까지 제거한 후에 , 그 값을 데이터프레임에 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away look jews treating races power unfortunate\n"
     ]
    }
   ],
   "source": [
    "print(news_df['clean_doc'][0])\n",
    "# 잘되었나 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) TF-IDF 행렬 만들기 \n",
    "- 정상적으로 불용어까지 제거되었음을 확인하고 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]  # 불용어 존재하지 않음을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TfidfVectorizer 를 통해 단어 1,000개에 대한 TF-IDF 행렬을 만들것\n",
    "- 여기서는 상위 1,000개의 단어로 제한하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 토픽 모델링(Topic Modeling)\n",
    "\n",
    "- 절단된SVD (truncated SVD)를 사용해 20개의 차원으로 차원축소 할것.\n",
    "- 즉 , 20개의 토픽을 가졌다고 가정한 후에 토픽 모델링을 시도.\n",
    "- 토픽의 숫자는 n_components의 파라미터로 지정이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized',n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 svd_model.componets_는 앞서 배운 LSA에서 VT에 해당됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확하게 토픽의 수 t x 단어의 수의 크기를 가지는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'administration',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'agencies',\n",
       " 'agree',\n",
       " 'algorithm',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'analysis',\n",
       " 'angeles',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'anybody',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'atheism',\n",
       " 'atheists',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'background',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bible',\n",
       " 'bike',\n",
       " 'bios',\n",
       " 'bits',\n",
       " 'black',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'body',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boston',\n",
       " 'bought',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'business',\n",
       " 'cable',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'canada',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'char',\n",
       " 'character',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'chips',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'code',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colors',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'congress',\n",
       " 'connection',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'contact',\n",
       " 'contains',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'contrib',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'convert',\n",
       " 'copies',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'criminals',\n",
       " 'cross',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'default',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'deleted',\n",
       " 'department',\n",
       " 'described',\n",
       " 'description',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'details',\n",
       " 'detroit',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'directory',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'disks',\n",
       " 'display',\n",
       " 'distribution',\n",
       " 'division',\n",
       " 'doctor',\n",
       " 'door',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'draw',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effort',\n",
       " 'electronic',\n",
       " 'email',\n",
       " 'encryption',\n",
       " 'energy',\n",
       " 'enforcement',\n",
       " 'engine',\n",
       " 'entire',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'escrow',\n",
       " 'especially',\n",
       " 'europe',\n",
       " 'event',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'exist',\n",
       " 'existence',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'export',\n",
       " 'extra',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fairly',\n",
       " 'faith',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'family',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'father',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'firearms',\n",
       " 'floppy',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'font',\n",
       " 'fonts',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'format',\n",
       " 'forward',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'fully',\n",
       " 'function',\n",
       " 'functions',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'genocide',\n",
       " 'germany',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goal',\n",
       " 'goals',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'greatly',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'guns',\n",
       " 'guys',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heaven',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hockey',\n",
       " 'hold',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'independent',\n",
       " 'individual',\n",
       " 'info',\n",
       " 'information',\n",
       " 'input',\n",
       " 'inside',\n",
       " 'installed',\n",
       " 'instead',\n",
       " 'institute',\n",
       " 'insurance',\n",
       " 'intended',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interface',\n",
       " 'internal',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'involved',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'jews',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'jpeg',\n",
       " 'kept',\n",
       " 'keyboard',\n",
       " 'keys',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'language',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'library',\n",
       " 'license',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'local',\n",
       " 'logic',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lower',\n",
       " 'lunar',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'magazine',\n",
       " 'mail',\n",
       " 'mailing',\n",
       " 'main',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'manual',\n",
       " 'march',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'master',\n",
       " 'material',\n",
       " 'matter',\n",
       " 'matthew',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'member',\n",
       " 'members',\n",
       " 'memory',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'method',\n",
       " 'michael',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'miles',\n",
       " 'military',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'minutes',\n",
       " 'misc',\n",
       " 'mission',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'models',\n",
       " 'modem',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'months',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'motif',\n",
       " 'mouse',\n",
       " 'multi',\n",
       " 'multiple',\n",
       " 'muslim',\n",
       " 'names',\n",
       " 'nasa',\n",
       " 'national',\n",
       " 'natural',\n",
       " 'nature',\n",
       " 'near',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'network',\n",
       " 'news',\n",
       " 'newsgroup',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'nuclear',\n",
       " 'null',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'object',\n",
       " 'objective',\n",
       " 'obvious',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'official',\n",
       " 'ones',\n",
       " 'open',\n",
       " 'operation',\n",
       " 'opinion',\n",
       " 'opinions',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orbit',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'original',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'package',\n",
       " 'page',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'paper',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'passed',\n",
       " 'past',\n",
       " 'patients',\n",
       " 'paul',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'performance',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'physical',\n",
       " 'pick',\n",
       " 'picture',\n",
       " 'pittsburgh',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'playing',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'port',\n",
       " 'position',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'postscript',\n",
       " 'power',\n",
       " 'practice',\n",
       " 'present',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'print',\n",
       " 'printer',\n",
       " 'privacy',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'produce',\n",
       " 'product',\n",
       " 'products',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'property',\n",
       " 'protect',\n",
       " 'protection',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'public',\n",
       " 'published',\n",
       " 'purpose',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quote',\n",
       " 'radio',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'realize',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasons',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'reference',\n",
       " 'references',\n",
       " 'regarding',\n",
       " 'regular',\n",
       " 'related',\n",
       " 'release',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'remember',\n",
       " 'remote',\n",
       " 'reply',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'request',\n",
       " 'require',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'resource',\n",
       " 'resources',\n",
       " 'respect',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'results',\n",
       " 'return',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risk',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'room',\n",
       " 'round',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'satellite',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'screen',\n",
       " 'scsi',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serial',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'server',\n",
       " 'service',\n",
       " 'services',\n",
       " 'setting',\n",
       " 'shall',\n",
       " 'shipping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'shows',\n",
       " 'shuttle',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'site',\n",
       " 'sites',\n",
       " 'situation',\n",
       " 'size',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'social',\n",
       " 'society',\n",
       " 'software',\n",
       " 'sold',\n",
       " 'soldiers',\n",
       " 'solution',\n",
       " 'somebody',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'source',\n",
       " 'sources',\n",
       " 'south',\n",
       " 'soviet',\n",
       " 'space',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'speed',\n",
       " 'spirit',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'standards',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'state',\n",
       " 'stated',\n",
       " 'statement',\n",
       " 'states',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'stephanopoulos',\n",
       " 'steve',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'stream',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'subject',\n",
       " 'suggest',\n",
       " 'suggestions',\n",
       " 'summer',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'supported',\n",
       " 'supports',\n",
       " 'suppose',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'suspect',\n",
       " 'switch',\n",
       " 'systems',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'term',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'theory',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'tools',\n",
       " 'toronto',\n",
       " 'total',\n",
       " 'town',\n",
       " 'trade',\n",
       " 'traffic',\n",
       " 'transfer',\n",
       " 'tried',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'turks',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'types',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'unfortunately',\n",
       " 'unit',\n",
       " 'united',\n",
       " 'universe',\n",
       " 'university',\n",
       " 'unix',\n",
       " 'unless',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usenet',\n",
       " 'user',\n",
       " 'users',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'valid',\n",
       " 'value',\n",
       " 'values',\n",
       " 'vancouver',\n",
       " 'various',\n",
       " 'vehicle',\n",
       " 'version',\n",
       " 'versions',\n",
       " 'video',\n",
       " 'view',\n",
       " 'voice',\n",
       " 'volume',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'washington',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'ways',\n",
       " 'weapon',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'went',\n",
       " 'west',\n",
       " 'western',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'widget',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'wire',\n",
       " 'wish',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'xterm',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()  #단어의 집합이며 , 1,000개의 단어가 저장되있다\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LSA의 장단점(Pros and Cons of LSA)\n",
    "\n",
    "- 장점\n",
    "    - LSA는 쉽고 빠르게 구현 가능\n",
    "    - 단어의 잠재적인 의미를 이끌어 유사도 계산에 좋은 성능 이끔\n",
    "- 단점\n",
    "    - 이미 계산된 LSA에 새로운 데이터 추가하여 계산하려고하면 처음부터 다시계산해야함\n",
    "    - 새로운 정보에 대해 업데이트 어려움\n",
    "\n",
    "이러한 단점으로 인해 LSA 대신에 Word2Vec 등 단어의 의미를 벡터화 할 수 있는 또다른 방법론인 인공 신경망 기반의 방법론이 각광받고 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
